\section{Results}

%"We don't care much about results, except when they're bad." - Ã–lle

% save me I'm useful somewhere
% \begin{equation}\label{rmse}
% RMSE(y,\hat{y}) = \sqrt{\frac{1}{n}  \sum_{i=1}^{n} (y_i - \hat{y}_i)}
% \end{equation}

Each model was trained for at most fifty epochs on the full training dataset. Early runs were trained with a 90/10 training-validation split, there didn't seem to be much room for improvement based on the separation between the validation and training loss (figure \ref{32b-106-90-10}). Thus, an 80/20 split was settled on. Early stopping would be invoked if validation loss did not improve for five epochs. To make sure the results could be equally quantified the dataset was shuffled with the same random seed each time.

\subsection{Benchmarks}\label{benchmarks}
The Galaxy Zoo Kaggle competition has a test image set of \numprint{79975} images. After training the model the network uses these images to produce predictions. These predictions are saved as a CSV file and uploaded to Kaggle. Kaggle then scores the predictions using the root mean square error (RMSE) between the model predictions and what the actual values are, (these actual values for the test set are kept secret) the lower scores are better. There are certain basic thresholds which can be used to determine if your model behaves as a random classifier or worse. The first is an All Zeros benchmark, this is equivalent to a network that always outputs zeros for all attributes. Similarly, there is an All Ones benchmark; these two benchmarks represent random classifications which, while not entirely trivial to pass, require some rational architectural choices. The final benchmark is harder, a Central Pixel benchmark, which are the predictions if the outputs were simply the central pixel RGB value in each training image. Very early implementations of the VGG16 model could pass the All Zero benchmark, later--with the right hyper-parameters-- the model could pass the All Ones and Central Pixel benchmarks. Results through the rest of this paper are based on the RMSE score Kaggle given for a particular model.

\subsection{Performance}

Before immediately attempting data-augmentation methods, a few hyperparameter configurations were run to get a better idea of the best input size and the best batch size for moving forward.

\begin{table}[]
    \centering
    \begin{tabular}{|r|c|c|}
        \hline
                      & \isize{69} & \isize{106} \\ \hline
        Batches of 16 & 0.11538 & 0.11062 \\ \hline
        Batches of 32 & 0.11476 & 0.11288 \\ \hline
    \end{tabular}
    \caption{Spread of results for various input and batch sizes. In the Kaggle competition this would put us in 78, 88, 94, 95 place respectively.}
    \label{tab:results1}
\end{table}

Results for the images were \isize{69} and where the batch size was 32 or 16 the loss graph and validation loss exhibited traits of overfitting (figure \ref{32b-69}). The results from Kaggle may in fact back this up, as the score gets better the larger the batch size, although two points don't necessarily make a trend. It could be that larger batch sizes may be necessary when the images are smaller.

\pngfigure{32b-69}{The training loss line being lower than the validation loss line characteristic of overfitting.}

\pngfigure{32b-106-90-10}{A run with a 90/10 training-validation split, the score from this run was admissible but ultimately beat by 80/20 splits.}

% This figure feels a little redundant
\pngfigure{32b-106-80-20}{A run with 80/20 training-validation split. Notice the smoother curve versus a run with the same parameters but a 90/10 split.}

\subsection{HSV Input format}

In digital imagery pixels are stored as combinations of three values red, green, and blue (RGB). However, there are many more ways to represent color; in printing for example: cyan, magenta, black (CMYK) are optimal values because of how light reflects on printed surfaces. Hue, saturation, value (HSV) is a different color space where the color is stored in hue, color intensity is stored in saturation, and the brightness is stored in a value. There were hypothesis if converting the galaxy images to this color format might enhance responses in the network, stars being bright, the star pixels will emit a high $V$ value and thus create stronger weights.

The results from this color space conversion were not terrific. With a batch of 32 with images the image size of \isize{106} got a score of $0.13778$ worse than any RGB result. The score was worse due to two issues over which there was much rumination. While $V$ may have been regularly high for the features we're trying to detect, $H$ may have been too varied for the network to converge on. For $S$, the color intensity of images was not very high, and so most colors were close to zero on the saturation scale. The combination of a rapidly fluctuating variable $H$, a regularly small value of $S$ made $V$ comparatively worthless to the network. This score would have earned 178^{th} in the Kaggle competition.

\subsection{Data Augmentation}
After the examining the results on 32 and 16 batches we decided to move forward with applying data augmentation to image sizes of \isize{106}.

\begin{table}[]
    \centering
    \begin{tabular}{|r|c|}
        \hline
                      & \isize{106} \\ \hline
        Batches of 16 &  0.11331 \\ \hline % 91
        Batches of 32 &  0.11673 \\ \hline % 97
    \end{tabular}
    \caption{Spread of results for various batch sizes with data augmentation. Augmentation was a random combination of reflection on the horizontal or vertical axis and being rotated by 90 degree increments. }
    \label{tab:results2}
\end{table}

This reinforces a forming hypothesis that smaller batch sizes for the \isize{106} image sizes are more optimal. These scores would have earned 91st and 97th respectively. There may be too much augmentation going on and not enough hyperparameter adjustment. As section \ref{aug} mentions, there are almost a million images with these transformations, it might have been best to perform some learning rate studies, for as is, the network might not have taken the large strides it needed to to get a better score than the un-augmented datasets.