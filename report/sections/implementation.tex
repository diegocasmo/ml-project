\section{Implementation}

% Data pre-processing, CNN setup, training, test, overfitting, data augmentation...

\subsection{Preprocessing}

% data augmentation, cropping & downsampling => dimensionality reduction

\subsection{VGG-16 Architecture}

\newcommand{\anet}{AlexNet\xspace}
\newcommand{\gnet}{GoogLeNet\xspace}
\newcommand{\inet}{ImageNet\xspace}

VGG-16 is a pre-trained 16 layer convolutional neural network used for image recognition. Built by the visual geometry group at Oxford in \citeyear{vgg16-arxiv} VGG-16 got second place to Google's \gnet in the Large Scale Visual Recognition Challenge (ILSVRC) 2014 \cite{vgg16-arxiv}. With the trained model released to the public, it is possible to build on top of the weights that are already set by the initial training phase. The advantage behind this is that the model doesn't need to be trained from scratch, this is a form of transfer learning. VGG-16 was trained on 1000 categories of images (classes).

Other pre-trained CNN architectures exist such as \gnet and \anet. \gnet uses average pooling instead of fully connected layers \cite{googlenet-paper} and beat VGG with an error rate of 6.67\% over VGG's 6.8\%. \anet won ILSVRC in 2012 and was novel for stacking multiple convolutional (conv) layers in a row before pooling their results \cite{alexnet-paper}. % so why is VGG advantageous again?

VGG is a clear choice over \gnet and \anet because the weights from the competition are available to the public for use and \anet and \gnet are not as readily available. Furthermore, VGG's fully connected nature without any special layers and widgets like \anet or \gnet make implementation and management much more simple. % hypothesis

\begin{table}
    \begin{center}
        \begin{tabular}{| c |}
        \hline
        Input $224 \times 224$ \\
        \hline
        Conv3-64 \\
        Conv3-64 \\
        \hline
        maxpool\\
        \hline
        Conv3-128 \\
        Conv3-128 \\
        \hline
        maxpool \\
        \hline
        Conv3-256 \\
        Conv3-256 \\
        Conv3-256 \\
        \hline
        maxpool \\
        \hline
        Conv3-512 \\
        Conv3-512 \\
        Conv3-512 \\
        \hline
        maxpool \\
        \hline
        Conv3-512 \\
        Conv3-512 \\
        Conv3-512 \\
        \hline
        maxpool\\
        \hline
        FC 4096\\
        \hline
        FC 4096\\
        \hline
        FC 1000\\
        \hline
        softmax output\\
        \hline
        \end{tabular}
        \caption{VGG-16 architecture, a line between each item represents a weighted layer of which there are 16. VGG also has 11, 13,and 19 weight layer variants which use fewer convolutional layers.}
    \end{center}
\end{table}

% pooling sizes
% convolution sizes
% advantages over AlexNet, and ImageNet

% Architecture changes we have to make
VGG-16 was trained on 1000 categories of images (classes), the Galaxy Zoo dataset has an expected output dimensionality of 37, by removing the last layer and inserting a new one with a size of 37 neurons the experiment can take advantage the pre-trained deep structure of VGG-16. Similarly, there need to be changes to the input layer of VGG-16, because the preprocessing methods crop and down-sample from $424 \times 424$ pixels to $106 \times 106$ pixels the input layer needs to be replaced with one of size $106 \times 106 \times 3$ (three for each color channel of the image: red, green, blue). Another change made for this problem was changing the output layer activation function from softmax to sigmoid.

\subsection{Software and Hardware}
For the software implementations there are two readily available and widely used machine learning frameworks Keras and TensorFlow. Keras is a high-level machine learning library written in Python. It can be used with a range of backend drivers which makes it very portable. TensorFlow is the backend driver we used...

A NVidia GeForce 970 graphics processing unit (gpu) was available, it was used for training each of the models described in results section. GPU's allow for embarrassingly parallel computations and Neural networks can be placed in that domain of being embarrassingly parallel. Using a GPU decreased training time by an order of magnitude.
