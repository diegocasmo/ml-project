\section{Implementation}

% Data pre-processing, CNN setup, training, test, overfitting, data augmentation...

\subsection{Data Preprocessing}

All images in the dataset are of size $424\times424$ and the object of interest is always centered. In order to reduce the dimensionality of the images, during the pre-processing images are cropped to $212\times212$, half their original size, images are also down sampled, discarding unnecessary information in each image which could impair the network. Down sampling can help the CNN learn which regions are related to each specific expression as well as perform on the GPU more efficiently \cite{deep-learning-review}.
% data augmentation, cropping & downsampling => dimensionality reduction

\subsection{CNN Architecture - VGG16}

\newcommand{\anet}{AlexNet\xspace}
\newcommand{\gnet}{GoogLeNet\xspace}
\newcommand{\inet}{ImageNet\xspace}

VGG-16 is a 16 weight-layer convolutional neural network used for image recognition. Created by the visual geometry group at Oxford in \citeyear{vgg16-arxiv} VGG-16 got second place to Google's \gnet in the Large Scale Visual Recognition Challenge (ILSVRC) 2014 \cite{vgg16-arxiv}. With the trained model released to the public, it is possible to build on top of the weights that are already set by the initial training phase. The advantage behind this is that the model doesn't need to be trained from scratch, this is a form of transfer learning. VGG-16 was trained on 1000 categories of images (classes).

Other pre-trained CNN architectures exist such as \gnet and \anet. \gnet uses average pooling instead of fully connected layers \cite{googlenet-paper} and beat VGG with an error rate of 6.67\% over VGG's 6.8\%. \anet won ILSVRC in 2012 and was novel for stacking multiple convolutional (conv) layers in a row before pooling their results \cite{alexnet-paper}. % so why is VGG advantageous again?

VGG is a clear choice over \gnet and \anet because the weights from the competition are available to the public for use and \anet and \gnet are not as readily available. Furthermore, VGG's fully connected nature without any special layers and widgets like \anet or \gnet make implementation and management much more simple. % hypothesis

\begin{table}
    \begin{center}
        \begin{tabular}{| c |}
        \hline
        Input $224 \times 224$ \\
        \hline
        Conv3-64 \\
        Conv3-64 \\
        \hline
        maxpool\\
        \hline
        Conv3-128 \\
        Conv3-128 \\
        \hline
        maxpool \\
        \hline
        Conv3-256 \\
        Conv3-256 \\
        Conv3-256 \\
        \hline
        maxpool \\
        \hline
        Conv3-512 \\
        Conv3-512 \\
        Conv3-512 \\
        \hline
        maxpool \\
        \hline
        Conv3-512 \\
        Conv3-512 \\
        Conv3-512 \\
        \hline
        maxpool\\
        \hline
        FC 4096\\
        \hline
        FC 4096\\
        \hline
        FC 1000\\
        \hline
        softmax output\\
        \hline
        \end{tabular}
        \caption{VGG-16 architecture, a line between each item represents a weighted layer of which there are 16. VGG also made 11, 13,and 19 weight-layer variants which use different counts of convolutional layers \cite{vgg16-arxiv}.}
    \end{center}
\end{table}

% pooling sizes
% convolution sizes
% advantages over AlexNet, and ImageNet

% Architecture changes we have to make
While VGG-16 was trained on 1000 categories of images (classes), the Galaxy Zoo dataset has an expected output dimensionality of 37. Therefore, several changes had to be made to the network architecture to support the inputs and ouputs of our experiments. The last layer is set to 37 neurons. On the input layer of VGG-16, because the preprocessing methods crop and down-sample from $424 \times 424$ pixels to $106 \times 106$ pixels the input layer needs to be one of size $3 \times 106 \times 106$ (three because the images have three color channels: red, green, blue). Another change made for this problem was changing the output layer activation function from softmax to sigmoid. With this new network architecture the pretrained weights from the ILSVRC competition released to the public by VGG were unusable, this meant training from scratch was necessary. Fortunately the hardware to do such training was available (see \ref{hard-soft-ware}).

\subsection{Hyperparameters}

% learning rate, activation functions, rprop optimization method, batch sizes.
We adopted a learning rate of \numprint(0.000001) from the original VGG paper.
Validation splits 95/5, 90/10, 80/20.
Batch sizes 16, 24, 32.

\subsection{Software and Hardware}\label{hard-soft-ware}
For the software implementations there are two readily available and widely used machine learning frameworks Keras and TensorFlow. Keras is a high-level machine learning library written in Python. It can be used with a range of backend drivers which makes it very portable. TensorFlow is the backend driver we used with Keras which allows access low level hardware for training.

A NVidia GeForce 970 graphics processing unit (GPU) was available, it was used for training each of the models described in results section. GPU's allow for embarrassingly parallel computations and neural networks can be placed in that domain of being embarrassingly parallel. Using a GPU decreased training time by an order of magnitude over CPU's.
