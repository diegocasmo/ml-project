\section{Results}

%"We don't care much about results, except when they're bad." - Ã–lle

% save me I'm useful somewhere
% \begin{equation}\label{rmse}
% RMSE(y,\hat{y}) = \sqrt{\frac{1}{n}  \sum_{i=1}^{n} (y_i - \hat{y}_i)}
% \end{equation}

Each model was trained for at most fifty epochs on a 90/10 training/validation split of the full dataset. Early stopping would be invoked if validation loss did not improve for five epochs. To better quantify results the dataset was shuffled with the same random seed each time.

\subsection{Benchmarks}
The Galaxy Zoo Kaggle competition has a test image set of \numprint{79975} images. After training the model the network uses these images to produce predictions. These predictions are saved as a CSV file and uploaded to Kaggle. Kaggle then scores the predictions using the root mean square error (RMSE) between the model predictions and what the actual values are (these actual values for the test set are kept secret) the lower scores are better. There are certain basic thresholds which can be used to determine if your model behaves as a random classifier or worse. The first is an All Zeros benchmark, this is equivalent to a network that always outputs zeros for all attributes. Similarly, there is an All Ones benchmark; these two benchmarks represent random classifications which, while not entirely trivial to pass, require some rational architectural choices. The final benchmark is harder, a Central Pixel benchmark, which are the predictions if the outputs were simply the central pixel RGB value in each training image. Very early implementations of the VGG16 model could pass the All Zero benchmark, later--with the right hyper-parameters-- the model could pass the All Ones and Central Pixel benchmarks. Results through the rest of this paper are based on the RMSE score Kaggle given for a particular model.

\subsection{Performance}

A few input configurations were tried.