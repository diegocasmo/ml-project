\section{Discussion}

From the get-go, there were a lot of models and solutions for this problem available online to trawl over and gain knowledge from. In that sense shoulders of giants were stood upon, and what tall giants they were. To summarize, a sixteen layer CNN was implemented in Keras with Tensorflow and was trained on a dataset of tens of thousands of images. These images were cropped and down-sampled to reduce dimensionality and make them a manageable size for the network. From those initial scores a series of other methods were tried and explored from data-augmentation to color space shifting. In the end the initial trials were the most successful, a full table of the results is available in table \ref{tab:all_results}.

\subsection{Further Work}
A top 100 score on Kaggle is an accomplishment but there is plenty of improvement to be done, the scores are in the bottom quartile after all. Hyperparameter studies beyond batch sizes and training splits were not performed, it is likely that hyperparameters such as learning rate and learning rate decay could have helped improve the network past current thresholds. A few props were taken from the winning Kaggle submission, what else could be garnered from their solution? A smaller network for one, feeding images of \isize{69} through such a large network might have been the folly of those runs which ultimately led to overfitting, there were simply too many network parameters and not enough input parameters to fill them. Follies with data-augmentation may not have been negligible either, while the images were certainly rotationally invariant, too many translations may have been performed and made the apparent dataset too large for the network to settle on. Tweaking the learning rate and number of epochs could have worked well, though in that scenario --with resources limited as they were-- there might not have been time to sufficiently explore that line of experimentation which would have involved running 8-12 hour jobs. Ultimately, many runs with varying methods producing varying results --instead of trying to laser in on a single great score by perpetually improving a single good result-- may have given a breadth but not depth of material to present.

With the network as it is, could there be other astronomical problems to tackle? In the coming years the James Webb space telescope will soon be launched (or hopefully, it has already been delayed multiple times this decade). An infrared telescope, it will be able to see objects in the Universe which conventional visible light telescopes cannot. This could open up whole new bodies previously obscured by dust and nebulae. Notable here, it will be able to detect the earliest galaxies in the Universe.

% what else?

\subsection{Contributions}
Prior work has demonstrated the effectiveness of machine learning techniques applied to astronomical datasets. This is further demonstrated here with a sufficiently large CNN which uses a very low learning rate. Present results indicate that data-augmentation may not work well with such a low learning rate and that other hyperparameters --such as learning-rate decay and batch sizes-- may be open to adjustment. The present research is therefore intended to make contributions towards the study of applications of machine learning on large, image based, astronomical datasets.

% useful for something?
% Diego maintained organization and wrote the initial code for training and validation splitting. Ankur gathered the many valuable research materials and implemented early versions of the \vgg architecture. Tristan finalized the implementations and ran the multi-hour experiments on the GPU. Everyone wrote a roughly equal amount of sections in this paper.

\subsection{Acknowledgments}

Gustaf Borgstr√∂m, the teaching assistant assigned to us, for keeping us on schedule and advising on the structure of this paper.
