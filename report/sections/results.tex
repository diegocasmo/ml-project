\section{Results}

%"We don't care much about results, except when they're bad." - Ã–lle

% save me I'm useful somewhere
% \begin{equation}\label{rmse}
% RMSE(y,\hat{y}) = \sqrt{\frac{1}{n}  \sum_{i=1}^{n} (y_i - \hat{y}_i)}
% \end{equation}

Each model was trained for at most fifty epochs on the full training dataset. Early runs were trained with a 90/10 training-validation split, there didn't seem to be much room for improvement based on the separation between the validation and training loss (figure \ref{32b-106-90-10}). Thus, an 80/20 split was settled on. Early stopping would be invoked if validation loss did not improve for five epochs. To make sure the results could be equally quantified the dataset was shuffled with the same random seed each time.

\subsection{Benchmarks}
The Galaxy Zoo Kaggle competition has a test image set of \numprint{79975} images. After training the model the network uses these images to produce predictions. These predictions are saved as a CSV file and uploaded to Kaggle. Kaggle then scores the predictions using the root mean square error (RMSE) between the model predictions and what the actual values are, (these actual values for the test set are kept secret) the lower scores are better. There are certain basic thresholds which can be used to determine if your model behaves as a random classifier or worse. The first is an All Zeros benchmark, this is equivalent to a network that always outputs zeros for all attributes. Similarly, there is an All Ones benchmark; these two benchmarks represent random classifications which, while not entirely trivial to pass, require some rational architectural choices. The final benchmark is harder, a Central Pixel benchmark, which are the predictions if the outputs were simply the central pixel RGB value in each training image. Very early implementations of the VGG16 model could pass the All Zero benchmark, later--with the right hyper-parameters-- the model could pass the All Ones and Central Pixel benchmarks. Results through the rest of this paper are based on the RMSE score Kaggle given for a particular model.

\subsection{Performance}

Before immediately attempting data-augmentation methods, a few hyperparameter configurations were run to get a better idea of the best input size and the best batch size for moving forward.

\begin{table}[]
    \centering
    \begin{tabular}{|r|c|c|}
        \hline
                      & $69 \times 69$ & $106 \times 106$ \\ \hline
        Batches of 16 & 0.11538 & 0.11062 \\ \hline
        Batches of 32 & 0.11476 & 0.11288 \\ \hline
    \end{tabular}
    \caption{Spread of results for various input and batch sizes. In the Kaggle competition this would put us in 78, 88, 94, 95 place respectively.}
    \label{tab:results1}
\end{table}

Results for the images were $69 \times 69$ and where the batch size was 32 or 16 the loss graph and validation loss exhibited traits of overfitting. The results from Kaggle may in fact back this up, as the score gets better the larger the batch size, although two points don't necessarily make a trend. It could be that larger batch sizes may be necessary when the images are smaller.

\pngfigure{32b-69}{The training loss line being lower than the validation loss line characteristic of overfitting.}

\pngfigure{32b-106-90-10}{A run with a 90/10 training-validation split, the score from this run was admissible but ultimately beat by 80/20 splits.}

\subsection{HSV Input format}

In digital imagery pixels are stored as combinations of three values red, green, and blue (RGB). However, there are many more ways to represent color; in printing for example: cyan, magenta, black (CMYK) are optimal values because of how light reflects on printed surfaces. Hue, saturation, lightness (HSL) is a different color space where the luminosity of an image is converted to a lightness value. There were hypothesis if converting the galaxy images to this color format might enhance responses in the network, stars being bright, the star pixels will emit a high $L$ value and thus create stronger weights. Here are the results, here's why it (probably did not I've tried HSV) work.

\subsection{Data Augmentation}
After the examining the results on 32 and 16 batches we decided to move forward with applying data augmentation to image sizes of $106 \times 106$.

\begin{table}[]
    \centering
    \begin{tabular}{|r|c||}
        \hline
                      & $106 \times 106$ \\ \hline
        Batches of 16 &  TODO \\ \hline
        Batches of 32 &  TODO \\ \hline
    \end{tabular}
    \caption{Spread of results for various batch sizes with data augmentation, this reinforces a hypothesis that smaller batch sizes for the $106 \times 106$ image sizes is more optimal. In the Kaggle competition...}
    \label{tab:results2}
\end{table}