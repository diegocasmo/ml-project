\section{Theory}

\subsection{Neural Network}

% What is a NN?
% How are they structured?
A standard neural network (NN) consists of many simple, connected processors called neurons. Each neuron produces a sequence of real-valued activations. A neuron can be activated from an input or another neuron's activation through its weighted connections from a previous layer \cite{NeuralNetwork}.

% Visual explanation of a NN
Figure \ref{fig:neural-network} shows how a NN receives a number of inputs which are connected to an activation function through weighted edges. A NN can have any number of inputs, hidden layers, or outputs.

\begin{figure} \label{fig:neural-network}
\centering
\begin{tikzpicture}[
  mycircle/.style={
     circle,
     draw=black,
     fill=white,
     inner sep=4pt,
     minimum size=20pt},
  mylabel/.style={font=\bf},
  myarrow/.style={-Stealth},
  node distance=1.5cm and 2cm
  ]
  \node[mylabel] (z1) {$z_1$};
  \node[mylabel,below=0.75cm of z1] (z2) {$z_2$};
  \node[mylabel,below=0.25cm of z2] (z3) { };
  \node[mylabel,below=1cm of z3]    (zn) {$z_I$};

  % neuron and output
  \node[mycircle, right=of z3] (neuron) {$f(net - \theta)$};
  \node[mylabel,  right=of neuron] (out) {$o$};

% edges
\draw [myarrow] (z1) -- node[above] {$v_1$} (neuron);
\draw [myarrow] (z2) -- node[below] {$v_2$} (neuron);
\draw [myarrow] (zn) -- node[below] {$v_I$} (neuron);

\draw [myarrow] (neuron) -- node[sloped] {} (out);

\path[] (z2) edge[thick, dash pattern=on \pgflinewidth off 3pt] node[] {} (zn);

\end{tikzpicture}
\caption{A NN which receives the inputs $z_1, z_2, ..., z_I$ and connects them to the neuron through the weighted edges $v_1, v_2, .., v_I$. The input values are multiplied by their respective weights and subtracted from a threshold ($\theta$). The result is passed to an activation function which produces the output \cite{Engelbrecht}.}
\end{figure}

% What is an activation?
% Define ReLU
An activation function computes a weighted sum of its input, adds a bias and decides whether the neuron's value should be propagated or not. A common choice for an activation function is the Rectified Linear Unit (ReLU) \cite{Relu}. A ReLU is defined as: $\sigma(x) = \max(0, x)$, where $x$ is the weighted sum of the inputs. It returns an output $x$ if $\sigma(x)$ is positive, $0$ otherwise. NN's using primarily ReLU activation functions have been shown to enable faster training even with many layers \cite{cnn-star-galaxy}.

\begin{figure}\label{fig:relu}
\centering
\begin{tikzpicture}[y=.4cm, x=0.4cm]
  %axes
  \draw [<->] (-10,0) -- coordinate (x axis mid) (10,0);
  \draw [->]  (0,0) -- coordinate (y axis mid) (0,10);

  %ticks
  \foreach \x in {-8,-6,...,8}
    \draw (\x,1pt) -- (\x,-3pt)
    node[anchor=north] {\x};

  \foreach \y in {2,4,...,8}
    \draw (1pt,\y) -- (-3pt,\y)
      node[anchor=east] {\y};

  % line
  \draw[blue] (-10,0) -- coordinate (x axis mid) (0,0);
  \draw[blue,thick, ->] (0,0) -- coordinate (y axis mid)(10,10);
\end{tikzpicture}
\caption{A ReLU function \cite{reluimage}.}
\end{figure}

It is possible for ReLU to introduce dead neurons whose output is always zero, this problem is known as the dying ReLU problem. To mitigate this issue, a slightly different version of ReLU known as leaky ReLU can be used. Leaky ReLU is defined in equation \ref{leaky-relu}.

\begin{equation}
\sigma(x) =
\begin{cases} \label{leaky-relu}
    x      , & \text{if } x\geq 0\\
    0.01x , & \text{otherwise}
\end{cases}
\end{equation}

Difference between the actual and predicted output (the error) is computed by loss function. In a NN, a loss function is used to correct weights after a feed forward operation, this process of correction is called back propagation. The error is propagated backwards from the output layer through the hidden layers to the input layer, wherein the weights and biases are modified in such a way that the error for the most recent input is minimized. Over many training samples the loss function will minimize error and the value of the weights for the whole network will begin to converge. The curve which the error rate of the network experiences is controlled through a gradient descent method. This method can help determine whether the network should be trained further or to stop early.

% TODO: more about gradient descent and early stopping, mention local minima

\subsection{Deep Learning}
% What is deep learning?
Deep learning is a set of algorithms in machine learning that attempt to learn in multiple levels, corresponding to different levels of abstraction. It is based on learning several levels of representations, corresponding to a hierarchy of features, where higher-level concepts are defined from lower-level ones, and the same lower level concepts can help to define many higher-level concepts. It typically uses artificial neural networks. An observation (e.g., an image) can be represented in many ways (e.g., a vector of pixels), but some representations make it easier to learn tasks of interest through examples (e.g., is this the image of a human face?). Research in this area attempts to define what makes better representations and the best methods for a NN to approach learning them \cite{DeepLearning}.

\subsection{CNNs}

% What is a CNN?
% What was it inspired by?
% When was this architecture created?
A CNN is a type of deep feed-forward neural network~\cite{cnn-star-galaxy} which is able to extract elementary visual features from its input. The creation of CNNs was motivated by Hubel and Wiesel's discovery in~\cite{hubel-wiesel-receptive-fields}, where they were able to find that a cat's visual cortex has locally sensitive, orientation-selective neurons. CNNs were first introduced in \citeyear{Lecun99objectrecognition} \cite{Lecun99objectrecognition}, and since then have been applied to solve numerous different type of problems in natural language processing \cite{Collobert:2008:UAN:1390156.1390177}, image recognition \cite{cnn-star-galaxy}, and recommendation systems \cite{NIPS2013_5004}.

% What is the input of CNN?
% What is a convolution?
% What are CNNs made of?
% What are convolutional layers?
% What are filters?
% What are feature maps?
A CNN takes an image as an input and feeds it through several layers; usually convolutional layers with ReLU activation functions, pooling layers, and a fully-connected layer (FCL). Convolutions are the primary operation of a CNN and what makes them distinct from other type of networks. A convolutional layer parameters is made up of a set of small learnable weights known as filters. A filter has a local receptive field, and given an image as an input to a convolutional layer, it convolves each filter across the image's width and height using a specified stride size and produces outputs called feature maps \cite{cnn-star-galaxy}. Filters are what allow a CNN to learn to extract visual clues from its input such as edges, lines, and corners \cite{Lecun99objectrecognition}. Equation \ref{cnn:feature-map} from \cite{cnn-star-galaxy} gives the mathematical definition of a feature map $k$. The summation is performed over the set of input feature maps, the symbol $*$ describes the convolution operator, and $\boldsymbol{x}_m$ the filters.

\begin{equation} \label{cnn:feature-map}
y^k = \sigma{\bigg(\sum_{m} \boldsymbol{w}^{k}_{m} * \boldsymbol{x}_m + b^k \bigg)}
\end{equation}

% Why ReLU as activation functions?
The filters of a CNN compute linear element-wise multiplication and additions to create the feature maps. In order to add non-linearity, it is common for the convolutional layers to use ReLUs as the activation function which additionally allow for faster training in networks with many layers \cite{cnn-star-galaxy}.

% What is a pooling layer?
% How does a pooling layer work?
% How does a CNN reduce dimensionality?
Feature maps are then fed through pooling layers. A pooling layer is typically of size~$2 \times 2$~\cite{NIPS2012_4824}, and its job is to essentially reduce the resolution of the previous feature map. A pooling layer performs an operation such as selecting the maximum value within its range, thus acting as a regularization technique to avoid overfitting.

% Why a fully-connected layer?
Finally, convolutional and pooling layers are followed up by a FCL. The FCL size needs to be the same as the number of classes or outputs the network has to learn to identify~\cite{Ciresan11flexible}, and its goal is to simply act as a classification layer which outputs the probability for each class.

% How are CNNs trained?
Once the architecture of a CNN has been specified, its filters and weights are initialized to small random values. Next, given an image as an input, it is fed through the convolutional, pooling operations, and the FCL. The output probabilities of the network are then used to compute the total error, and finally gradient descent is used to update the filter and weight values with respect to their contribution to the total error. This process is repeated with other images in the dataset until satisfactory results are achieved.
