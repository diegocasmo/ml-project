\section{Implementation}

% Data pre-processing, CNN setup, training, test, overfitting, data augmentation...

\subsection{Data Preprocessing}

All images in the dataset are of size $424\times424$ and the object of interest is always centered. In order to reduce the dimensionality of the images, during preprocessing images are cropped to $212\times212$, half their original size, images are also down sampled to half size again $ 106\times 106$, discarding unnecessary information in each image which could impair the network. Down sampling can help the CNN learn which regions are related to each specific expression as well as improve performance when training \cite{deep-learning-review}.
% data augmentation, cropping & downsampling => dimensionality reduction

\subsubsection{Data Augmentation}\label{aug}
While there are a sufficient count images to train on, it is possible to increase the performance of the network by augmenting the training data. Because the galaxies are rotationally invariant this dataset is an ideal candidate for applying data augmentation to. Therefore, a random transformation was performed online to different images across epochs. This transformation could be any combination of rotation by 90 degree increments or a flip on the horizontal or vertical axes, or no transformation at all (one in four chance that there is no rotation at all). This brings 16 possible configurations for each image. Trained over 50 epochs there is a good likelihood that the network will see every configuration of any given image.

\subsection{CNN Architecture - VGG16}

\newcommand{\anet}{AlexNet\xspace}
\newcommand{\gnet}{GoogLeNet\xspace}
\newcommand{\inet}{ImageNet\xspace}

\newcommand{\isize}[1]{$#1 \times #1$}

VGG-16 is a 16 weight-layer convolutional neural network used for image recognition. Created by the visual geometry group at Oxford in \citeyear{vgg16-arxiv} VGG-16 got second place to Google's \gnet in the Large Scale Visual Recognition Challenge (ILSVRC) 2014 \cite{vgg16-arxiv}. With the trained model released to the public, it is possible to build on top of the weights that are already set by the initial training phase. The advantage behind this is that the model doesn't need to be trained from scratch, this is a form of transfer learning.

Other pre-trained CNN architectures exist such as \gnet and \anet. \gnet uses average pooling instead of fully connected layers \cite{googlenet-paper} and beat VGG with an error rate of 6.67\% over VGG's 6.8\%. \anet won ILSVRC in 2012 and was novel for stacking multiple convolutional (conv) layers in a row before pooling their results \cite{alexnet-paper}. % so why is VGG advantageous again?

\begin{wraptable}{r}{4cm}
    \centering
    \begin{tabular}{| c |}
        \hline
        Input $224 \times 224$ \\
        \hline
        Conv3-64 \\
        Conv3-64 \\
        \hline
        maxpool\\
        \hline
        Conv3-128 \\
        Conv3-128 \\
        \hline
        maxpool \\
        \hline
        Conv3-256 \\
        Conv3-256 \\
        Conv3-256 \\
        \hline
        maxpool \\
        \hline
        Conv3-512 \\
        Conv3-512 \\
        Conv3-512 \\
        \hline
        maxpool \\
        \hline
        Conv3-512 \\
        Conv3-512 \\
        Conv3-512 \\
        \hline
        maxpool\\
        \hline
        FC 4096\\
        \hline
        FC 4096\\
        \hline
        FC 1000\\
        \hline
        soft-max output\\
        \hline
    \end{tabular}
    \caption{VGG-16 architecture \cite{vgg16-arxiv}.}
    \label{vgg-table}
\end{wraptable}

% pooling sizes, ponvolution sizes
Table \ref{vgg-table} lays out the full VGG-16 architecture. VGG also made 11, 13, and 19 weight-layer variants which use different counts and sizes of conv layers. For the variant implemented here, all conv layers are of size \isize{3} and have a stride of one. The second number after "conv3" in table \ref{vgg-table} represents the number of image channels, which increase as the architecture gets deeper and the image input more filtered.

% advantages over AlexNet, and ImageNet
VGG is a good choice over \gnet and \anet because the weights from the competition are available to the public for use and \anet and \gnet are not as readily available. Furthermore, VGG's fully connected nature without any special layers and widgets like \anet or \gnet make implementation and management much more simple. % hypothesis

% Architecture changes we have to make
While VGG-16 was trained on 1000 categories of images (classes), the Galaxy Zoo dataset has an expected output dimensionality of 37. Therefore, several changes had to be made to the network architecture to support the inputs and ouputs of the experiments. The last layer is set to 37 neurons. On the input layer of VGG-16, because the preprocessing methods crop and down-sample from \isize{424} pixels to \isize{106} pixels the input layer needs to be one of size $3 \times 106 \times 106$ (three because the images have three color channels: red, green, blue). Another change made for this problem was changing the output layer activation function from soft-max to sigmoid. With this new network architecture the pre-trained weights from the ILSVRC competition released to the public by VGG were unusable, this meant training from scratch was necessary. Fortunately the hardware to do such training was available (see \ref{hard-soft-ware}).

\subsection{Dropout}
Dropout is a method to prevent neural networks from overfitting \cite{dropout}. It does this by randomly deactivating some neurons in one --or many-- layers. This makes the remaining active neurons compensate their weights twice as much in the back-propagation phase. Neurons are randomly activated and deactivated per update; here, each time a batch has been fully fed through the network. By activating and deactivating neurons in layers, dropout keeps the neurons of a network robust to the training data by never letting them fully fit to the training data. In \cite{dropout} the authors recommend to keep dropout ratio between $0.5$ and $1$. In the implementation of VGG-16 the 50\% dropout rate --which the original implementors chose-- was unchanged. In this VGG-16 implementation dropout only applies to the final two FCL layers. This means that during any given batch only 2048 neurons are actually active in each layer.

\subsection{Hyperparameters}

% learning rate, activation functions, rprop optimization method, batch sizes.
A learning rate of $10^{{-6}}$ was a adopted, this is much lower than the original VGG-16 learning rate, which was trained with a learning rate of $10^{{-2}}$ and was decreased by a factor of 10 when the validation accuracy stopped improving \cite{vgg16-arxiv}. This decrease was because the dataset was much smaller, the ImageNet training dataset is $1.3$ million images, a higher learning rate would be advantageous with that much data because you would want to take very large improvements in the beginning. Momentum of $0.9$ was used. There was no learning rate decay. Early validation splits were very high, at 90\% training and 10\% validation (90/10) this split was producing models which were overfit as evidenced by poor performance as well as the training loss on the model becoming --sometimes significantly-- less than the validation loss. Later, this was changed to a more traditional validation split of 80/20. The test set was provided entirely separately by Kaggle and is discussed in section \ref{benchmarks}.

%Batch sizes 16 and 32.
Dimensions of the images can be reduced dramatically and still be useful to the network. Two image sizes were tried, \isize{106} and \isize{69}. The \isize{106} image size is the center of the image cropped to \isize{212} and downsampled. The \isize{69} size was prompted from the winning submission to the Kaggle competition, the author crops at 207 and then downsamples 3x to \isize{69} \cite{kaggle-winner}. Different batch sizes were tried too: 16, 32. These batch sizes determine how fast the weights of the network get updated. The smaller the batches get to one the closer the network gets to using simple gradient descent methods. Because the dataset and the number of parameters in the network are both large, adjusting weights with an extreme frequency could lead to getting stuck in minima or over-adjusting and missing the target function.

Another change from the presented VGG-16 implementation was the output layer activation function. We chose sigmoid over soft-max, this was an important change because the soft-max function has difficulty predicting hard zeros and ones \cite{kaggle-winner}. Throughout the dataset there are many hard zero values which represent the galaxy not having that attribute at all.

\subsection{Software and Hardware}\label{hard-soft-ware}
For the software implementations there are two readily available and widely used machine learning frameworks Keras and TensorFlow. Keras is a high-level machine learning library written in Python. It can be used with a range of backend drivers which makes it very portable. TensorFlow is a backend driver used with Keras which allows access low level hardware for training.

A NVidia GeForce 970 graphics processing unit (GPU) was available, it was used for training each of the models described in results section. GPU's allow for embarrassingly parallel computations and neural networks can be placed in that domain of being embarrassingly parallel. Using a GPU decreased training time by an order of magnitude over traditional CPU's.
