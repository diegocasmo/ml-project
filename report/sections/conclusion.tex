\section{Discussion}

From the get-go, there were a lot of models and solutions for this problem available online to trawl over and gain knowledge from. In that sense shoulders of giants were stood upon, and what tall giants they were. To summarize, a sixteen layer CNN was implemented in Keras with Tensorflow and was trained on a dataset of tens of thousands of images. These images were cropped and down-sampled to reduce dimensionality and make them a manageable size for the network. From those initial scores a series of other methods were tried and explored from data-augmentation to color space shifting. In the end the initial trials were the most successful. 

\subsection{Further Work}
A top 100 score on Kaggle is an accomplishment but there is plenty of improvement to be done. Hyperparameter studies beyond batch sizes and training splits were not performed, it is probably that parameters like learning rate and learning rate decay could have helped improve the network past current thresholds. Essentially, the problem was many runs with varying methods producing varying results; instead of trying to laser in on a single great score by perpetually improving a single good result. A few props were taken from the winning Kaggle submission, what else could be garnered from their solution? A smaller network for one, feeding images of \isize{69} through such a large network might have been the folly of those runs which ultimately led to overfitting, there were simply too many network parameters and not enough input parameters to fill them. Follies with data-augmentation may not have been negligible either, while our images were certainly rotationally invariant , too many translations may have been performed and made the apparent dataset too large for the network to settle on. Tweaking the learning rate and number of epochs could have worked well, though in that scenario with resources limited as they were, there might not have been time to sufficiently explore that line of experimentation which would have involved running 8-12 hour jobs.

With the network as it is, could there be other astronomical problems to tackle? In the coming years the James Webb space telescope will soon be launched (or hopefully, it's already been delayed multiple times this decade). An infrared telescope, it will be able to see objects in the universe which conventional visible light telescopes cannot. This could open up whole new bodies previously obscured by dust and nebulae. Notable here, it will be able to detect the earliest galaxies in our universe. The network could probably classify cats too.

\subsection{Contributions}
Diego kept us organized and wrote initial code for training and validation splitting. Ankur wrote and gathered the many valuable research materials. Tristan coded and babysat the multi-hour runs. Everyone wrote lots in this paper.